step 0: train loss 4.2874, val loss 4.2823
iter 0: loss 4.2664, time 18774.79ms, mfu -100.00%
iter 10: loss 3.2446, time 84.29ms, mfu 4.42%
iter 20: loss 2.7914, time 87.61ms, mfu 4.40%
iter 30: loss 2.6379, time 90.59ms, mfu 4.38%
iter 40: loss 2.5755, time 89.45ms, mfu 4.35%
iter 50: loss 2.5265, time 92.11ms, mfu 4.32%
iter 60: loss 2.5135, time 89.73ms, mfu 4.31%
iter 70: loss 2.4943, time 88.72ms, mfu 4.30%
iter 80: loss 2.4974, time 87.93ms, mfu 4.29%
iter 90: loss 2.4696, time 88.97ms, mfu 4.28%
iter 100: loss 2.4622, time 88.22ms, mfu 4.27%
iter 110: loss 2.4542, time 89.60ms, mfu 4.26%
iter 120: loss 2.4236, time 89.55ms, mfu 4.25%
iter 130: loss 2.4183, time 90.66ms, mfu 4.24%
Traceback (most recent call last):
  File "/workspaces/nanoGPT/train.py", line 305, in <module>
    scaler.scale(loss).backward()
  File "/home/codeany/.local/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/codeany/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/codeany/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
