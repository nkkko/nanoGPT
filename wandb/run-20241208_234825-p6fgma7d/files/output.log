step 0: train loss 4.2873, val loss 4.2823
iter 0: loss 4.2632, time 102931.24ms, mfu -100.00%
iter 10: loss 3.2226, time 667.67ms, mfu 1.12%
iter 20: loss 2.7756, time 669.48ms, mfu 1.12%
iter 30: loss 2.6236, time 669.48ms, mfu 1.12%
iter 40: loss 2.5427, time 675.06ms, mfu 1.11%
iter 50: loss 2.5190, time 673.79ms, mfu 1.11%
iter 60: loss 2.4823, time 676.33ms, mfu 1.11%
iter 70: loss 2.4861, time 676.17ms, mfu 1.11%
iter 80: loss 2.4622, time 679.32ms, mfu 1.11%
iter 90: loss 2.4523, time 677.67ms, mfu 1.11%
iter 100: loss 2.4374, time 678.55ms, mfu 1.11%
iter 110: loss 2.4466, time 677.07ms, mfu 1.11%
Traceback (most recent call last):
  File "/workspaces/nanoGPT/train.py", line 323, in <module>
    lossf = loss.item() * gradient_accumulation_steps
KeyboardInterrupt
